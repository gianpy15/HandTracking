Handtracking - from data collection to system deployment
L. Cavalli, G. Di Pietro, M. Biasielli, M. Bertoni, M. Di Fatta

Abstract - In this paper we propose the full-stack development of a realtime hand-tracking system. The final objective of the system is to be able to track a 3D model of 21 distinct joints of a hand from realtime captured video frames running on common hardware. We must highlight that realtime handtracking is not a novel task: state of the art models <PUT HERE REFERENCES TO PAPERS> achieve very good tracking accuracy but can run in realtime only on very high end hardware. In this project we encompass the whole development stack: data collection, massive data labeling, model ideation and training, hyperparameter selection, performance assessment, deployment and on-field testing.

Introduction
Realtime handtracking has the potential to become a new disruptive pervasive human-machine interface, but in order to be really adopted on a large scale it needs to lower its computational resource requirements. Current approaches display a very good tracking accuracy even in the case of occlusions, but need to fully employ very expensive hardware.
Our objective in this work is to achieve decent resource consumptions while keeping decent accuracy, trying different approaches with the freedom given by a full-stack development. At the time of this writing the work is still in progress, and the whole development process has been completed only for what concerns the task of locating the hands in the image (bounding boxes), while no model for the joints identification displayed decent accuracy up to now.

Document Structure
In this document we first detail the description of every stage of the development: data collection, data labeling, model selection and training, hyperparameter selection, performance assessment, deployment, on-field testing. A section about the overall system architecture idea will follow.

Data Collection
A very good review of the available datasets is presented by <REFERENCE TO THE AUTHORS AND WORK OF https://arxiv.org/pdf/1704.02612.pdf>. Most of the available datasets of RGB-D labeled images of hand joints offers frames captured in few different settings and from few repeated subjects, using a variable number of joints and resolution. Although there is a good variety of them, we decided to build our own dataset in order to be able to label and consider different unconventional aspects of frames like the direct visibility or occlusion of a particular joint, being also able to reconstruct long sequences of connected frames with a well known framerate. The explicit visibility information for example would enable fast estimation refinement of the final 3D model with depth information, while guarantees on framerate would help building more stable time-recurrent models. This does not exclude the use of ready-made datasets in some stages of the work: we indeed used egohands <LINK TO EGOHANDS WEBSITE> to augment our hand localization dataset and synthands <LINK TO SYNTHANDS WEBSITE> to test it.
To build our dataset we used an Intel RGB-D camera <USE SPECIFIC MODEL NAME> and an ad-hoc modified version of its drivers to be able to either save (for actual data collection) or stream (for realtime later model testing) the captured RGB-D frames in known formats. We used different structures to fix it and shoot short videos of one hand from ego, third person view and subject <SMALL REFERENCE ON FOOT TO EXPLAIN THAT THE HAND WAS THE MAIN SUBJECT OF THE VIDEO, JUST IN FRONT OF THE CAMERA> perspective, with either open, clean or cluttered backgrounds. We made sure to have hands of different shapes and colors, moving at different speeds. A minority of videos was captured also while manipulating small objects causing additional hand occlusions. Some videos were captured using a clean blue background in order to be able to easily find and cut the hand on each frame to be pasted on a different background. We also registered outdoor videos in different light conditions and with no hand at all to be used just as animated backgrounds for blue-screened hands. Our dataset currently holds <NUM VIDEOS> videos for a total of <NUM FRAMES> labelled frames overall.

Data Labeling
For each captured frame we need to be able to reconstruct all the information content about the hand. The main description is a sequence of 21 joint locations, one for the wrist position and four for each finger; for each joint we need to know its 2D location on the image and its depth value, hence the need for the occlusion information in order to be able to distinguish a correct depth value on the D image from a wrong one. Some secondary potentially useful information is the fact that the hand is right or left, seen by palm or back; right-left distinction is very easy to label as it is a property of the entire video and can be diffused to each frame, while palm-back can be geometrically inferred from the right-left distinction and the joint 2D locations, in particular if the angle between index and pinky is right-handed on a right hand or left-handed on a left hand the hand is seen by the palm, otherwise it's seen by the back side.
The information to be gathered for each frame is its joint 2D locations and occlusion. For this reason we built a small website <REFERENCE TO LINK https://goo.gl/ByXGf2> to ask for help in labeling. On this website users can receive a sample of our choice, zoom on it and select each one of the 21 joint locations distinguishing between visible and not visible. When performing manual labeling it is important that the user has best possible visual feedback of the data he is providing to us in order to reduce the intrinsic noise that will be generated by different people, for this reason a skeletal model of the hand appears gradually while each joint is inserted with different colors for each finger to give an immediate idea of how the described hand would appear. In the end they can submit the labelled frame by a nickname and eventually get one more. There is a publicly available ranking of the nicknames counting the most contributions, we owe great thanks to anybody who helped <LINK TO /thanks.php TO DISPLAY THE TOP CONTRIBUTORS>.
Manual labeling is a very long process that can provide labels only for a limited number of frames. In order to take the best out of it, we organized and indexed all frames to optimize the chosen frame to propose to the next user each time. Between two labeled frames, all unlabeled frames can be temporarily labelled by linearly interpolating the two extreme labels. This is possible because of the continuity of the space of our labels, and can be extended also to the occlusion flags modeling the uncertainty of the occlusion itself. The nearest the two extreme frames, the most accurate the interpolation will be. For this reason each time a frame is requested we browse all indices of each video to choose the frame that minimizes the maximum interval between two unlabeled frames. This way we can get acceptable results even by labeling one frame every four, and good results labeling only half of the frames.
One more peculiar problem of manual labeling is the intrinsic noise over the produced labels, and the presence of some evident outliers in the labels due to human errors. We could find numerous examples of samples which were completely missed (mainly before the introduction of the skeletal feedback on the website) and had to be removed or corrected. To address this problem we built a custom video player that allows us to play our videos at different speeds visualizing also all the direct and interpolated labels provided by users. Finding evident outliers is a fast operation playing the video in natural or even slower speed; once found the player enables us to either discard some frame labels to be requested to the next users or to move the label points and perform the interpolation with the new points. Sometimes it is also useful to slightly correct the interpolation itself on fast movements and to fix those frames as they are. When a video appears to have high quality data it can be fixed and never proposed to users anymore to give more space to others.
Labeled data is then converted into an intermediate format to enable merging it with different datasets and to optimize training speed. Floating point values whose precision is not essential are quantized into 8-bit integers to minimize the size of files and thus the time to read them when starting a training session. Moreover by standardizing data format we can merge more than one dataset into a single standard, uniform and larger source of data even when the needed information must be inferred by other datasets through complex procedures which must be run only once.

Model Selection and Training
As we require to be able to test the performance of many very different models, it is foundamental to have the possibility to deploy a fully featured training procedure of a new architecture in a short time. Our standard training procedure features automatic training and validation set separation to ensure that no video has frame in both sets, asynchronous prioritized data loading to perform a first training batch while data is still being read (usually disk I/O is the bottleneck of the system), early stopping with file checkpoints, arbitrary training-related image logging through tensorboard available realtime on the web, and mobile notification of training start and end events. All of these features are there to grant fast training that can be monitored and analized realtime, and time efficiency in deciding whether the model has some potential or should be stopped. For this reason we built a high level training structure on top of Tensorflow and Keras to adapt all of these features to any model and any data shape and need that allows us to be able to deploy the training of a completely new model usually in hours if not minutes.
<SUBTITLE> Heatmap loss </SUBTITLE>
Most of the models we trained were based on heatmaps to locate either the hand or its joints. As location heatmaps of small objects are prevalently black, a common problem of all these models is the very attractive local minimum of a fully black output. This is evident when using the classical loss for heatmaps, the RSS, as all models always ended up converging from random output to black output. In order to balance the contributions of black and white we elaborated a custom loss Lh 
<PUT FORMULA Lh = E_H[dm(p_e - p_gt)*w(p_gt, H_gt)] note that _ means subscript>
where E_H is the expected value over the whole heatmap, p_e is the estimated pixel, p_gt is the ground truth corresponding pixel and H_gt is the whole ground truth heatmap. The dm function is a general mapper of the estimation error difference, while the w function assigns an importance weight to each pixel based on the whole heatmap. The classic RSS loss can be obtained by this with the choices:
<PUT FORMULA w(p_gt, H_gt) = 1 , dm(epsilon) = epsilon^2 >
What we want is to weight each pixel proportionally to the square of its distance from the average of the heatmap w(p_gt, H_gt) <SYMBOL PROPORTIONAL TO> (p_gt-E[H_gt])^2. Here we introduce the real hyperparameter wp (white-priority) to gain control over the importance of the white against the black: we bias the average mu=E[H_gt] with the formula:
<PUT FORMULA mu_biased = sigmoid(-log(1/mu - 1) - wp) >
Notice that for wp = 0 we have mu_biased = mu. We then use mu_biased instead of mu to apply the bias on the loss. As it is the weighting function as an undesired side effect will give overall more importance also to heatmaps with more variance, thus weights must be normalized:
<PUT FORMULA w(p_gt, H_gt) = (p_gt - mu_biased)^2 / E[(p_gt - mu_biased)^2] =
	(p_gt - mu_biased)^2 / (Var(H_gt) + (mu - mu_biased)^2) >
Moreover to have explicit control over the different penalization of false positives and false negatives we propose an asymmetrif function dm depending on the real hyperparameter delta:
<PUT FORMULA dm(epsilon) = delta/2 * epsilon^3 + (1-delta/2) * epsilon^2 >
This ensures that false positives (epsilon = 1) weight one while false negatives (epsilon = -1) weight 1-delta.
<SUBTITLE> Best models </SUBTITLE>
Here follows a review of some of the models we tested so far:
Plain convnet palm/back classifier:
<DESCRIPTION>
High field of view plain convnet: 
<DESCRIPTION>
Eta-net: 
<DESCRIPTION>
Mobilenet transfer:
<DESCRIPTION>
Gradient injection net:
<DESCRIPTION>
Vector field joint inference:
<DESCRIPTION>
Joint regressor:
<DESCRIPTION>

Hyperparameter selection
Each one of our models depends on many hyperparameters ranging from the most classical learning rate, dropout rate to our custom loss parameters. Training over a decent amout of samples also takes time, so an automatic hyperparameter algorithm may be far too expensive and take far too much time to evaluate the best configuration for a given model. Our approach here has been expert based: in our tensorboard logged images we had samples from both training and validation set and the evolution of the network output over time during training in order to have the clues to diagnose the problems of the run and manually choose the needed change to hyperparameters. On each run we registered on a form the chosen hyperparameter values and the measured performances with both exact and fuzzy metrics to have a general overview of the explored points in the search space.
This step can be considered passed by a model only when proving acceptable performances on the validation set. We must also consider that the hyperparameter-dependent loss we employed could not be used at all to compare the performances of different configurations, thus tast-dependent metrics had to be designed each time.

Performance assessment
At this stage we want to evaluate a trained model with a test set and some comparable performance measure. We must notice that only the mobilenet transfer model <REFERENCE TO MOBILENET TRANSFER SECTION IN MODEL TRAINING PARAGRAPH> for hand location has come to this point.
To test the performances of a hand locating model we first searched for some standard measurement but no solid standard was found: the most common measurements were in avarage pixels of distance between target and estimated location. We then decided to do the same, but dividing by the square root of the number of pixels in order to have a normalized relative error.
To ensure that the test set is not at all correlated with the training and validation sets, we used a third different dataset, the synthands <LINK TO SYNTHANDS WEBSITE>, to assess our measurements. Our best transfer mobilenet model performed <VALUE PERFORMED... MAYBE COMMENT?>.

Deployment
A core factor to achieve cheaper realtime is the utilization of resources. Having a lighter model surely makes a system lighter, but we can act upon its working frequency to make it impact less. The modern trend on hardware growth is towards thread level parallelism, thus we must enclose our models into some abstraction layer that is able to dynamically increase throughput and hide latency through adaptive depth pipelining and output interpolation. In our system the model is enclosed into a scheduler that is requested from the outside to perform a forward inference with a given frequency. In order to maintain the frequency, the scheduler keeps a running average of the forward inference time and decides the depth of a multithreaded pipeline needed to keep that pace. If the requested frequency is too high and the processor starts to execute the threads in time sharing, the average execution time grows and triggers an adaptive reduction of the model execution frequency to bring the pipeline back to real parallelism. The outputs of each run are labeled with their reference time and collected for interpolation. The output interface of this abstraction layer is the interpolation module that collects the most recent N samples and uses them to answer requests of the values of the function at specific time instants, using linear interpolation with polynomial features of order M. Using low order polynomial features makes the interpolation procedure very cheap and still even an accurate predictor when the latency is not too high and the function is not too irregular over time. Through pipelining we increase throughput up to the parallel capabilities of the host machine and through interpolation we implement the abstraction of a continuously available and cheap function that can be queried for any time instant at any moment, even the running present, thus hiding latency at the cost of accuracy.

On-field testing
As a final system-wide validation step, and for the flavour of touching the result of our work, we developed a small script to employ a deployed module for hand localization onto the stream of images coming from the webcam. The script streams the images of the webcam and tries to place a bounding box around the object that seems most like a hand on the visible screen. The stream of images itself is very fluid as it is not blocked by the inference time of the hand localization network, while the bounding box placement is at times noisy but correct overall, and <VERIFY> can reach a working frequency of 60 fps at full load for a latency of 0.15s on an average laptop, completely masked by linear interpolation </VERIFY>. Operating at lower frequencies than full load and masking the lower granularity with cheap interpolation lets the whole machine to operate other tasks more fluidly, which is our end-objective.

Overall System Architecture Idea
By overall system architecture concept we mean the final structure that the working system will have. This may change at any time, but here we present an established idea. The working system should have some input image stream, this stream is the input source for a deployed module for locating hands and their dimensions. From each located hand a small set of lightweight features should be extracted to be able to recognize it again in the very next frame, and a time-coherent ID should be assigned. Sampling over time on this layer should provide identified crops of the corresponding image, one for each recognized hand, each with relocation information. A subsequent module should be able to recognize the 2D joint locations and visibility of each hand crop: with relocation information we can then reposition the joints into the original 2D image space and a 3D model in camera space of the hand skeleton can be built. <A PICTURE REPRESENTING ALL OF THIS WOULD BE NICE>
We experimented several methodologies for 3D reconstruction, and the best performing is through geometrical interpolation. We need to start from the basic model of some hand at rest in standard coordinates to have a starting position with correct proportions, this model can be a standard fixed model or the result of some calibration procedure on the user's hand: the better the model, the better the reconstruction accuracy. The objective of the model reconstruction is to find the transformations (wrist translation and rotations of mobile joints of the hand) that minimize the discrepancy between the image-projected joint positions and the inferred 2D joint positions. We considered the inverse problem of finding the transformations that project the model points at the minimum distance from the rays projected from the origin and corresponding to each 2D image estimated joint position.
The first step is fitting the basic rigid triangle between wrist, base index and base pinky with fixed side lenghts between the three corresponding rays: this problem can be solved numerically and has four solutions, reducible to two by central mirroring: these two solutions correspond to the hand facing up or down and can be both found numerically by restarting the procedure several times.
Once the three base points have been found the corresponding base translation and rotation matrix can be easily computed to move all the other points of the model to position. Each finger rotation again can be found by a constrained nonlinear optimization problem: fixed the center of the rotation, the direction around which the joint can rotate and the plane on which the main rotation is performed, we look for the rotations on the plane and perpendicular to the plane that minimize the distance of the relative point on the sphere from the projected line of the corresponding point estimation, keeping the angle into constrained intervals to allow only natural positions. This problem again can be solved numerically for each mobile joint, starting from the solution to the unconstrained relaxed problem which can be easily solved analitically. Also when the plane-perpendicular allowed angles are small enough (as for the terminal tip finger joints) we can drop a dimension of the search space, allowing much faster execution. <HERE AGAIN PICTURES WOULD HELP SO MUCH>
The obtained model can finally be refined with depth values when the joint is labeled as visible: when the measured depth is near the estimated camera distance it is accepted as a more accurate estimation, otherwise it is considered an outlier and rejected.
The python implementation of this procedure <VERIFY> takes around 0.03s on an avarage laptop with no parallelization at all </VERIFY>, but it can be improved by parallelizing the computation of each finger. Its output can be interpolated, so that it can be easily enclosed into the deployment module described in its section <REFERENCE TO DEPLOYMENT SECRION>


